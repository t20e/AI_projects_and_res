{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f60f0e6",
   "metadata": {},
   "source": [
    "# Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1268947",
   "metadata": {},
   "source": [
    "Softmax is an activation function that converts a vector of raw, real-valued numbers (logits) into a probability distribution. Each value in the output probability distribution is between 0 and 1, and all values sum to 1.\n",
    "\n",
    "\n",
    "\n",
    "- It's the standard output activation for multi-class classification problems.\n",
    "\n",
    "\n",
    "- The neuron with the highest probability represents the model's final prediction.\n",
    "\n",
    "- Example: For an input image, the model might output logits $[1.2, 2.9, 0.4]$. After applying softmax, these become probabilities like [0.18, 0.81, 0.01] for the classes ['cat', 'dog', 'bird']. The model's prediction is 'dog' since it has the highest probability (0.81).\n",
    "\n",
    "  \n",
    "$$Softmax(Z)_i = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}$$\n",
    "\n",
    "_Where:_\n",
    "\n",
    "- $Z$ =  The vector of input logits for a single sample (e.g., [1.2, 2.9, 0.4] -> mapped to -> ['cat', 'dog', 'bird']).\n",
    "\n",
    "- $z_i$: The i-th element in the vector $z$. This single value is the raw score (logit) for the $i'th$ class.\n",
    "  - Example: If your classes are ['cat', 'dog', 'bird'], then $z_1$ is the score for 'cat', $z_2$ is the score for 'dog', and so on...\n",
    "\n",
    "* $K$ is the number of classes, in our example above we have 3 animals.\n",
    "\n",
    "* $e^{z_i}$ The standard exponential function applied to each logit. This makes all values positive.\n",
    "* $\\sum_{j=1}^K e^{z_j}$ The normalization term. It's the sum of all the exponentiated logits, which ensures the final probabilities sum to 1.\n",
    "\n",
    "**Numerical Stability:**\n",
    "- Directly calculating $e^{z_i}$ can be a problem. If your logits are large $(e.g., 1000)$, $e^{1000}$ is an astronomically large number that causes a numerical overflow (an inf error). To fix this, we use a stabilization trick by subtracting the maximum logit from all logits before exponentiating.\n",
    "\n",
    "$$Softmax(Z)_i  = \\frac{e^{z_i - max(z)}}{\\sum_{j=1}^K e^{z_j - max(z)}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7df3727c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def softmax(logits):\n",
    "    \"\"\"Apply softmax\n",
    "\n",
    "    Args:\n",
    "        logits: The output of the last layer\n",
    "    \"\"\"\n",
    "    # Subtract the max for numerical stability\n",
    "    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "    # Normalize to get probabilities\n",
    "    return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf3f4978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5003897 , -1.93858329,  1.42608783],\n",
       "       [ 4.05780403, -0.65232111,  1.97923639],\n",
       "       [ 1.85062418, -2.6290737 ,  0.25195059],\n",
       "       [-3.9246835 , -1.30803736, -1.34155564]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Example Usage ---\n",
    "\n",
    "# Let's simulate a batch of 4 samples for the 3 classes\n",
    "num_images = 4\n",
    "num_classes = 3 # Corresponds to ['cat', 'dog', 'bird']\n",
    "\n",
    "# The output of the previous layer (the logits)\n",
    "# Shape: (num_images, num_classes)\n",
    "logits = np.random.randn(num_images, num_classes) * 2 # Multiply by 2 for more varied logits\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6796d5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Logits:\n",
      " [[ 0.5003897  -1.93858329  1.42608783]\n",
      " [ 4.05780403 -0.65232111  1.97923639]\n",
      " [ 1.85062418 -2.6290737   0.25195059]\n",
      " [-3.9246835  -1.30803736 -1.34155564]]\n",
      "--------------------\n",
      "Probabilities after Softmax:\n",
      " [[0.27694081 0.0241632  0.69889599]\n",
      " [0.8817464  0.00793894 0.11031466]\n",
      " [0.82406173 0.00934225 0.16659602]\n",
      " [0.03580608 0.49017573 0.47401818]]\n",
      "--------------------\n",
      "Sum of probabilities for each sample:\n",
      " [1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Logits:\\n\", logits)\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# Apply the softmax function\n",
    "probabilities = softmax(logits)\n",
    "\n",
    "print(\"Probabilities after Softmax:\\n\", probabilities)\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# Verify that each row (sample) sums to 1\n",
    "print(\"Sum of probabilities for each sample:\\n\", np.sum(probabilities, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39939451",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
