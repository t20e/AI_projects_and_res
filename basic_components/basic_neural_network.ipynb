{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0155d23",
   "metadata": {},
   "source": [
    "# Basic Neural Network\n",
    "\n",
    "âœ¨ Building an NN (Neural network) requires piecing together components together to perform a particular task.\n",
    "\n",
    "- Note that for the task below, since were only counting, a basic regression model could also be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e97104c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.basic_nn import draw_image_cell_image\n",
    "\n",
    "# Reloads external functions when their code changes into this notebook.\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "# %aimport will only reload those files\n",
    "# %aimport utils.bboxes\n",
    "%aimport utils.basic_nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d528f9",
   "metadata": {},
   "source": [
    "**Task: Create a NN (Neural Network) that can count the number of black cells in a image.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144eb7e9",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "- We have a $(3,3) = 9$ pixel image with black and white cells:\n",
    "    - A $0 =$ black cell, a $1 =$ white cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0645a679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0],\n",
       "       [1, 0, 1],\n",
       "       [1, 0, 1]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_array = np.array([\n",
    "    [1, 1, 0],\n",
    "    [1, 0, 1],\n",
    "    [1, 0, 1]\n",
    "])\n",
    "image_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa04c07",
   "metadata": {},
   "source": [
    "Plot what the image looks like. Ignore the purple lines there just to emphasize the cells, and the 0 and 1 are just for show."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63ebe1f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_image_cell_image(image_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d654ff1",
   "metadata": {},
   "source": [
    "### Preprocess The Dataset\n",
    "\n",
    "**Inputs And Labels:**\n",
    "- Inputs can also be noted as ****features**** or ****x****.\n",
    "    - The input is the image_array\n",
    "    - Each pixel is a single feature.\n",
    "    - The input is what we will train the model on.\n",
    "    - Reshape the (3x3) image array into a (9, 1) vector, so we can pass it through the model.\n",
    "\n",
    "- Labels can also be noted as **target**, ****ground truth**** or ****y****.\n",
    "    - When the model makes output predictions, we will compare it to the label, the more it matches the better our model is trained on that dataset.\n",
    "  \n",
    "    - Classes (num_classes=10):\n",
    "        - Class 0: The image has 0 black cells.\n",
    "        - Class 1: The image has 1 black cell.\n",
    "        - ...\n",
    "        - Class 9: The image has 9 black cells.\n",
    "    - One-hot encode:\n",
    "      - We need to create a one-hot encoded label to represent the label\n",
    "        - Ex: $[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]$\n",
    "          - The label indicates $4$ is how many black cells are in the image, since $1$ is in the third index (zero-based indexing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab576146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "One-hot label for a count of 3:\n",
      " [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "numBlackCells = np.count_nonzero(image_array == 0)\n",
    "\n",
    "# Create the one-hot encoded label\n",
    "num_classes = 10\n",
    "y = np.zeros((1, num_classes))\n",
    "y[0, numBlackCells] = 1\n",
    "\n",
    "print(\"\\nOne-hot label for a count of 3:\\n\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910815d5",
   "metadata": {},
   "source": [
    "Reshape the (3x3) image array into a (9, 1) vector, so we can pass it through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e06253e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened image input vector:\n",
      " [[1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "# Flatten the image array into a 1D vector of 9 features\n",
    "flatten_image_array = image_array.flatten().reshape(-1, 1)\n",
    "print(\"Flattened image input vector:\\n\", flatten_image_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4874131",
   "metadata": {},
   "source": [
    "### Normalize\n",
    "\n",
    "- For image datasets, to make the model train faster and converge better we normalize the image data pixel values to be between $[0,1]$, \n",
    "    - Regular **greyscale** values are:\n",
    "        - $0$ value represents black and $255$ value represents white pixel.\n",
    "- However, since our image is already made of just **0s and 1s**, it's already in a small, stable range. We can skip normalization for this dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd055b0",
   "metadata": {},
   "source": [
    "## Create A Basic NN Model\n",
    "\n",
    "**Prerequisite:**\n",
    "- [Softmax](./activation_functions/softmax.ipynb)\n",
    "\n",
    "**Network:**\n",
    "<p align=\"center\">\n",
    "    <img src=\"./showcase_images/network_with_10_outputs.png\" alt=\"logistic regression network\" width=\"80%\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a7d8dc",
   "metadata": {},
   "source": [
    "Network description:\n",
    "- The input layer has $9$ neurons one for each pixel then $x_n$ and $w_n$ would be $x_{9}$ and $w_{9}$\n",
    "    - $x_1$ = is the first pixel of the image.\n",
    "    - $x_2$ = is the second pixel.\n",
    "    - $x_{n} = x_{9}$ = is the $9'th$ pixel.\n",
    "\n",
    "    - $w_n = $ 9, to match the input features.\n",
    "\n",
    "- The output has $10$ neurons, one for each possible count (0 through 9).\n",
    "  - Example: An image can have a max of $9$ black pixels and a minimal of $0$ black pixels, we want the model to predict a number between $[0,9]$\n",
    "    - $[0.1, 0.0, 0.4, 0.0, 0.0, 0.1, 0.2, 0.2, 0.0, 0.0]$\n",
    "      - $0.4$ is the highest value the model predicted that. \n",
    "        - $0.4$ is in the second index (zero-based indexing) so that means the model predicts that the number of black pixels in the image is $2$.\n",
    "\n",
    "Train Steps:\n",
    "1. Forward propagation. Where we train the model\n",
    "2. Backward propagation. Where we update the model to make it make more acret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611d87c3",
   "metadata": {},
   "source": [
    "#### Learned Parameters\n",
    "\n",
    "`w` (weights) & `b` (bias).\n",
    "- `w:`\n",
    "    -  Each weight neuron determine the importance or influence of each input feature on the neuron's output. Each feature/pixel has a corresponding weight w. A large weight means that a change in the input will have a significant impact on the output.\n",
    "\n",
    "    - In this network each weight node is connected to every input neuron.\n",
    "\n",
    "    - The w has the same number of **neurons** as an input sample.\n",
    "        - A sample is one example of a dataset, for this example dataset, we only have one image, so we have one sample.\n",
    "\n",
    "    - Randomly initializing the weights. This is crucial to break symmetry, which ensures that each weight neuron learns a unique function.\n",
    "        - We are using the [Xavier Initialization](./param_initializers/Xavier_init.ipynb), but there are other initialization algorithms such as the [He algorithm](./param_initializers/He_init.ipynb). \n",
    "        - If all weights were initialized to the same value like $0$, all neurons in a layer would learn the exact same thing, making the network ineffective.\n",
    "\n",
    "- `b:`\n",
    "    - This is a constant value added to the linear combination of inputs and weights. It allows the model to shift the activation function curve to the left or right, independent of the input values. The bias essentially gives the neuron a degree of flexibility, allowing it to fire even if all inputs are zero or to not fire even if there are high inputs.\n",
    "    - Bias can be initialized to be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b362bbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights_and_bias(n_in=9, n_out=1):\n",
    "    \"\"\"\n",
    "    Initialize the weights and bias. Initializing the weights using the Xavier algorithm, set bias scalar to be zero.\n",
    "\n",
    "    Args:\n",
    "        n_in: The total number of input features/neurons, this dataset its 9 pixels.\n",
    "        n_out: The total number of output neurons, since this network is just a input to output network, meaning we have no layers, we use 1 n_out which then gets sigmoid applied to it, to get the models output.\n",
    "\n",
    "    returns:\n",
    "        w : weights array\n",
    "        b : scalar\n",
    "    \"\"\"\n",
    "    # Xavier Uniform Initialization, note theres also a Xavier Normal Initialization.\n",
    "    limit = np.sqrt(6 / (n_in + n_out))\n",
    "    # Create the weight matrix with random values from a uniform distribution\n",
    "    w = np.random.uniform(-limit, limit, size=(n_out, n_in))\n",
    "\n",
    "    # Initialize bias (b) to zeros\n",
    "    b = np.zeros((1, n_out))\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4363c567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.06495111, -0.55464097,  0.36804813,  0.47925215,  0.52715839,\n",
       "         -0.28084445,  0.52096595,  0.20128627, -0.0256567 ],\n",
       "        [-0.3516144 , -0.30104466,  0.20563233, -0.03103297,  0.40187344,\n",
       "          0.48877552,  0.47548505, -0.39950541,  0.26568792],\n",
       "        [-0.33191267,  0.26956716,  0.28342505, -0.27356339,  0.09376107,\n",
       "         -0.46267874,  0.04364748, -0.11047292,  0.44786794],\n",
       "        [-0.3973079 , -0.08742557,  0.05364788, -0.54560155, -0.15290978,\n",
       "          0.39185526, -0.42761787,  0.31210657, -0.09388759],\n",
       "        [ 0.1541925 ,  0.25519112,  0.17883092, -0.53416436, -0.52456117,\n",
       "         -0.25541772,  0.08643212, -0.09665057, -0.45765328],\n",
       "        [-0.44420232,  0.15138703, -0.09560852,  0.31813356,  0.45644259,\n",
       "          0.2794617 , -0.15958103, -0.49730656,  0.41348808],\n",
       "        [ 0.10322178,  0.32293196, -0.07608702,  0.13948773,  0.17968399,\n",
       "         -0.27300098, -0.23220257,  0.1204129 ,  0.16643176],\n",
       "        [-0.07384989,  0.26890124,  0.52105352, -0.48579264, -0.2402108 ,\n",
       "          0.01209187, -0.07392864,  0.48026374, -0.04129343],\n",
       "        [-0.09086983, -0.50735428, -0.37334031, -0.25059846, -0.45295178,\n",
       "          0.25540356,  0.10808401,  0.11670636, -0.47068147],\n",
       "        [ 0.12899852, -0.35720694,  0.07720997, -0.43464336,  0.28327953,\n",
       "         -0.08920056, -0.24248016, -0.47152155, -0.20957628]]),\n",
       " array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, b = init_weights_and_bias(n_in=9, n_out=10) # softmax function expects 10\n",
    "w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d153cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 9), (1, 10))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.shape, b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c2ea31",
   "metadata": {},
   "source": [
    "- The weights `w` have a shape of **(10, 9)**. This represents the 10 output neurons, each connected to all 9 input pixels. The bias `b` has a shape of **(1, 10)**, with one bias value for each output neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aa20b3",
   "metadata": {},
   "source": [
    "### Forward Propagation\n",
    "\n",
    "$$z^{(i)} = w * x^{(i)}+ b$$\n",
    "\n",
    "$$\\hat{y}^{(i)} = a^{(i)} = \\sigma(z^{(i)}) = sigmoid(z^{(i)})$$\n",
    "\n",
    "- **Notes:** \n",
    "    - $a^{(i)} $ & $ \\sigma(z^{(i)})$ & $sigmoid(z^{(i)})$ mean the same thing, its just different ways that you might see it annotated.\n",
    "    - $^{(i)}$ means its for one example/sample, the dataset we have only has 1 image, so that is our one sample:\n",
    "\n",
    "    - **Compute Linear**:\n",
    "      - $W$ is the weights.\n",
    "      - $b$ is the bias.\n",
    "      - Calculate the scores (logits) for each of the 10 classes\n",
    "\n",
    "    - **Compute non-linear activation:**\n",
    "      - Apply [$softmax()$](./activation_functions/softmax.ipynb) to the $Z$ to convert those (logits) scores into probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99625c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x, w, b):\n",
    "    \"\"\"\n",
    "    Performs a forward pass of the network.\n",
    "    \n",
    "    Args:\n",
    "        x: Input data, shape (9, 1)\n",
    "        w: Weights, shape (10, 9)\n",
    "        b: Bias, shape (1, 10)\n",
    "    \"\"\"\n",
    "    # Note: We transpose x to (1, 9) to match bias shape (1, 10)\n",
    "    # The dot product w @ x.T results in shape (10, 1)\n",
    "    # So we transpose the result to (1, 10) before adding the bias\n",
    "    z = (w @ x).T + b\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f33d7ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.20402709,  0.54625646, -0.30707221, -1.15998522, -0.75141963,\n",
       "         0.55868702,  0.22686968, -0.3938715 , -0.95601646, -1.20410878]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = linear(x=flatten_image_array, w=w, b=b)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fda1f25",
   "metadata": {},
   "source": [
    "**Activation Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b83819d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"Applies the softmax activation function.\"\"\"\n",
    "    # Subtract max for numerical stability\n",
    "    exp_z = np.exp(z - np.max(z))\n",
    "    return exp_z / np.sum(exp_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fd0eaba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13879191, 0.19543056, 0.08325236, 0.0354798 , 0.05338496,\n",
       "        0.19787503, 0.14199876, 0.07633085, 0.04350745, 0.03394834]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities = softmax(z)\n",
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652cb054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The highest probability is our predicted class (the count of black cells)\n",
    "predicted_count = np.argmax(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7a35529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Probabilities (y_hat):\n",
      " [[0.13879191 0.19543056 0.08325236 0.0354798  0.05338496 0.19787503\n",
      "  0.14199876 0.07633085 0.04350745 0.03394834]]\n",
      "\n",
      "Model predicted: 5 vs real number of black cells: 3\n"
     ]
    }
   ],
   "source": [
    "print(\"Output Probabilities (y_hat):\\n\", probabilities)\n",
    "print(\"\\nModel predicted:\", predicted_count, \"vs real number of black cells:\", numBlackCells)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5405b900",
   "metadata": {},
   "source": [
    "Now lets perform backpropagation to improve the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c048d10",
   "metadata": {},
   "source": [
    "### Backward Propagation\n",
    "backward propagation: use Stochastic Gradient Descent (SGD) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917aa0b8",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825136af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b13ca0d5",
   "metadata": {},
   "source": [
    "TODO Model performs terrible, but it has so far only seen the image once, lets make it see the image more than."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
