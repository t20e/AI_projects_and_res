{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b97200a",
   "metadata": {},
   "source": [
    "# YOLO Inference On Taco Images\n",
    "\n",
    "- TACO: (Trash Annotations in Context)\n",
    "\n",
    "* A model must be trained.\n",
    "    - Be located in the ./checkpoints dir.\n",
    "    - And its filename must be in config.yaml, ex: \"yolo_v1_taco_D_2025-07-08_EPOCH_50_LOSS_2.1525_S_448.pt\".\n",
    "* Run Yolo model on custom images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e210d8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from argparse import Namespace\n",
    "import torch.optim as optim\n",
    "from yolov1 import Train\n",
    "from utils.checkpoints import load_checkpoint\n",
    "from yolov1 import YOLOv1\n",
    "torch.set_printoptions(threshold=torch.inf) # When printing tensors, should all values, only use in Jupyter Notebook.\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189ca6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Jupyter Notebook: reloads external functions when its code changes.\n",
    "%load_ext autoreload\n",
    "%autoreload 1 \n",
    "# 0: Disables automatic reloading (default setting). 1: Reloads only modules imported using the %aimport magic command. 2: Reloads all modules (except those explicitly excluded by %aimport)\n",
    "\n",
    "# %aimport module name will only reload those files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168d17f6",
   "metadata": {},
   "source": [
    "### Add Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf41f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport utils.load_config\n",
    "from utils.load_config import load_config\n",
    "\n",
    "config = load_config()\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd5d4e3",
   "metadata": {},
   "source": [
    "### Add Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e75f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <------------- Transforms ------------->\n",
    "class Compose(object):\n",
    "    \"\"\"Apply a sequence of transforms safely on (image, bboxes).\"\"\"\n",
    "\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img, bboxes):\n",
    "        for t in self.transforms:\n",
    "            img, bboxes = t(img, bboxes)\n",
    "        return img, bboxes\n",
    "\n",
    "\n",
    "class Resize(object):\n",
    "    \"\"\"Resize the image. No change needed for bboxes since they are normalized (0-1).\"\"\"\n",
    "\n",
    "    def __init__(self, size):\n",
    "        self.size = size  # (width, height) like (448,448)\n",
    "\n",
    "    def __call__(self, img, bboxes):\n",
    "        img = T.Resize(self.size)(img)\n",
    "        return img, bboxes  # bboxes stay the same\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert image to Tensor. Leave bboxes as they are.\"\"\"\n",
    "\n",
    "    def __call__(self, img, bboxes):\n",
    "        img = T.ToTensor()(img)  # Automatically normalize image between 0-1\n",
    "        return img, bboxes\n",
    "\n",
    "\n",
    "transforms = Compose(\n",
    "    # transform object to resize the bboxes and images.  Normalize image tensors\n",
    "    [\n",
    "        Resize((448, 448)),  # Resize image to 448x448\n",
    "        ToTensor(),  # Convert image to tensor\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc32e955",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae716737",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo = YOLOv1(in_channels=3, S=config.S, B=config.B, C=config.C).to(config.DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    yolo.parameters(), lr=config.LEARNING_RATE, weight_decay=config.WEIGHT_DECAY\n",
    ")\n",
    "if config.CON_TRAINING:\n",
    "    load_checkpoint(file_name=config.LOAD_MODEL_FILE, yolo=yolo, optimizer=optimizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ea3ecc",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9438233",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolov1.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "# config.MODE = \"test\"\n",
    "dataset = Dataset(S=config.S, B=config.B, C=config.C, mode=config.MODE, dataset_path=config.DATASET_DIR, transforms=transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb27c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t, label_t = dataset.__getitem__(3) # Load the image at index __(int) from the dataframe csv. Depending on the config.MODE, the dataframe will either be train.csv, valid.csv or test.csv\n",
    "img_t.shape, label_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02306e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = to_pil_image(img_t)\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14342553",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c01981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a batch to the img tensor, so that it can be feed to the model. Yolo model expects shape (BATCH_SIZE, img_channel_size, Img_Size, Img_Size).\n",
    "img_t = img_t.unsqueeze(0)\n",
    "img_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9380a516",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = yolo(img_t.to(config.DEVICE))\n",
    "out, out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561a01c1",
   "metadata": {},
   "source": [
    "### Reshape Output\n",
    "* Reshape output from (1, 1372) -> (1, 7, 7, 28) ->  (7, 7, 28) \n",
    "    - Remove the batch because we are only working with one image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd5debb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape output\n",
    "out = out.view(1, 7, 7, 28)\n",
    "out = out.squeeze()\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b825fe9",
   "metadata": {},
   "source": [
    "### Extract Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce99a5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport utils.bboxes\n",
    "\n",
    "from utils.bboxes import extract_bboxes, reconstruct_tensor\n",
    "\n",
    "pred_bboxes = extract_bboxes(out, config) # (N, 9) [ i, j, b, class_idx, pc, x, y, w, h]\n",
    "pred_bboxes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ea7fc8",
   "metadata": {},
   "source": [
    "### Compute Non-Max-Suppression\n",
    "* Remove redundant bounding boxes from the models prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c315f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport utils.nms\n",
    "\n",
    "from utils.nms import non_max_suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c833c644",
   "metadata": {},
   "outputs": [],
   "source": [
    "nms_bboxes = non_max_suppression(\n",
    "    pred_bboxes=pred_bboxes,\n",
    "    config=config\n",
    ")\n",
    "# The bboxes from the models prediction that survived NMS.\n",
    "nms_bboxes.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb333c6",
   "metadata": {},
   "source": [
    "### Plot The Predicted And True Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8e9f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport utils.plot\n",
    "from utils.plot import plot_bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19be7d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_t = label_t.to(config.DEVICE)\n",
    "label_bboxes = extract_bboxes(label_t, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e600aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_filter = label_bboxes[label_bboxes[:, 4] == 1]\n",
    "label_filter # The bboxes from the label where an object is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71915ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bboxes(img=img, label_bboxes=label_filter, pred_bboxes=nms_bboxes, S=config.S)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466521d9",
   "metadata": {},
   "source": [
    "### Upload Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c093bb1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
