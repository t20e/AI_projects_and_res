{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# <------------- Hyperparameters/Config ------------->\n",
    "config = Namespace(\n",
    "    DEVICE = torch.device(\"mps\"), # apple silicon M series\n",
    "    NUM_WORKERS = 2,\n",
    "    PIN_MEMORY = True,\n",
    "    \n",
    "    EPOCHS = 1,\n",
    "    LEARNING_RATE = 2e-5,\n",
    "    BATCH_SIZE = 64,\n",
    "    WEIGHT_DECAY = 0,\n",
    "\n",
    "\n",
    "    LOAD_MODEL = False,\n",
    "    LOAD_MODEL_FILE = None,\n",
    "    \n",
    "    DATASET_DIR = \"./data\", # root path to the dataset dir\n",
    "    IMAGE_SIZE = 448,\n",
    "\n",
    "    C = 3, # how many classes in the dataset\n",
    "    B = 2, # how many bounding boxes does the model perdict per cell\n",
    "    S = 7, # split_size, how to split the image, 7x7=49 grid cells,\n",
    "    IOU_THRESHOLD = 0.5, # the iou threshold when comparing bounding boxes for NMS\n",
    "    MIN_THRESHOLD = 0.4 # the minimal confidence to keep a predicted bounding box\n",
    ")\n",
    "\n",
    "config.NUM_NODES_PER_CELL = config.C + 5 * config.B # The total number of nodes per cell, which would be the size ==> [with_mask, without_mask, mask_worn_incorrectly, pc_1, bbox1_x_y_w_h, pc_2, bbox2_x_y_w_h] = 13 nodes.\n",
    "config.NUM_NODES_PER_IMG = config.S * config.S * (config.C + config.B * 5) # number of nodes that each image has. If S=7 C=3 B= 2 ==+> 7*7 * (3 + 2 * 5) = 637, also 13 * 49 = 637"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(DEVICE=device(type='mps'), NUM_WORKERS=2, PIN_MEMORY=True, EPOCHS=1, LEARNING_RATE=2e-05, BATCH_SIZE=64, WEIGHT_DECAY=0, LOAD_MODEL=False, LOAD_MODEL_FILE=None, DATASET_DIR='./data', IMAGE_SIZE=448, C=3, B=2, S=7, IOU_THRESHOLD=0.5, MIN_THRESHOLD=0.4, NUM_NODES_PER_CELL=13, NUM_NODES_PER_IMG=637)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 7, 13])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 1  # or any number you want\n",
    "tensor = torch.randn(batch_size, 7, 7, 13)\n",
    "tensor.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.3619, -0.1905, -0.3031, -0.2435, -0.0026,  0.8839, -0.2493, -0.6624,\n",
       "         0.8513, -0.6011, -1.9173,  0.0658,  0.2680])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.3619, -0.1905, -0.3031])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor[0][0][0][..., :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "def generate_model_file_name(model_name, dataset_name, task, epochnum):\n",
    "    # <model_name>_<dataset>_<task>_<details>_<epoch|date|version>.pt\n",
    "    # example: resnet50_cifar10_classification_lr0.001_epoch20.pt\n",
    "    \n",
    "    date = datetime.now().strftime(\"%Y-%m-%d-%Hh_%Mm\")\n",
    "    return f\"{model_name}_{dataset_name}_{task}_epoch{epochnum}_{date}.pt\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yolov1_facemask_objectDetection_epoch12_2025-04-08-21h_51m.pt'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_model_file_name(\"Yolov1\", \"facemask\", \"objectDetection\", 12 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
