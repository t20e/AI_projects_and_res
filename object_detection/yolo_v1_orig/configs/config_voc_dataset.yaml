# ðŸ’¡ Project Configurations: For this project most attributes are hard-coded to match the original paper.

# NOTE: If you change the Key names please be sure to update the YOLOConfig class in ./configs/config_loader

MODE: "train" # 'train' or 'test' -> what are we doing?

# -------------------------------- DEVICE AND SYSTEM --------------------------------
DEVICE: "mps" # GPU device ->  "mps": Apple silicon M series | "cuda": Nvidia GPU | "cpu": if no GPUs available use CPU
NUM_WORKERS: 2 # Set to 0 will give you a better error trace when debugging. Default: 2.

# To speed up the data transfer to the GPU from CPU when using pytorch's DataLoader, you can set pin_memory:True, however it isn't supported on Mac Silicone yet.
PIN_MEMORY: False

# --------------------------------- HYPERPARAMETERS ---------------------------------
EPOCHS: 1 # Ex: epoch = 1 means the model sees the dataset 1 time.
LEARNING_RATE: 1e-6

BATCH_SIZE: 5 # The number of images per batch. It must be < NUM_TRAIN_SAMPLES if NUM_TRAIN_SAMPLES != 0.
VAL_BATCH_SIZE: 2 # Batch size for the validation dataset. Must be < NUM_VAL_SAMPLES if NUM_VAL_SAMPLES != 0.

# Note paper: used 'a momentum of 0.9 and a decay of 0.0005', I decided to use the Adam optimizer instead, this is a slight deviation from the paper.
MOMENTUM: 0.9
WEIGHT_DECAY: 0.0005

# ------------------------ CONTINUE TRAINING PRE-TRAINED MODELS ------------------------
CON_TRAINING: False # Boolean | continue to train a model

SAVE_MODEL: True # Should the model be saved. Set to False when writing code and we don't want to save the model.
# Custom file name to add to filename when saving checkpoint or trained models. Ex: "overfit_one_image"
CUSTOM_FILE_NAME: ""

# File name of model to load it can be either a checkpoint or a trained model.
LOAD_MODEL_FILE: "ttoverfit_one_image_yolo_v1_dataset_VOCDataset_date_2025-07-19_EPOCH_100_LOSS_1.5732_SIZE_448.pt" #EX: yolo_v1_dataset_VOC2012_train_val_date_2025-07-16_EPOCH_15_LOSS_4.9248_SIZE_448.pt
LAST_EPOCH: 0 # (int) The last epoch the model was trained on DON'T change from 0.

# ----------------------------------- ARCHITECTURE -----------------------------------
C: 20 # How many classes in the dataset
B: 2 # How many bounding boxes does the model predict per cell
S: 7 # split_size how to split the image, 7x7=49 grid cells
IOU_THRESHOLD: 0.6 # The threshold to remove or keep boxes when comparing predicted bounding boxes for NMS.
MIN_THRESHOLD: 0.5 # The minimal confidence score needed to keep a predicted bounding box.

LABEL_NODES: 1470 # config.S * config.S * (config.C + config.B * 5) === The total number of nodes that each label has for one image. If S=7 C=20 B=2 --> 7 * 7 * (C + B * 5) = 1470 | 7x7=49 -> 49*30Â =Â 1470 | the (* 5) adds the second bounding box in the cell -> [pc_2, x, y, w, h].
CELL_NODES: 30 # The total number of nodes that a single cell has in a label for one image. Which would be the size (C + 5 * B) -> [*classes, pc_1, bbox1_x_y_w_h, pc_2, bbox2_x_y_w_h]. If S=7 C=20 B=2 --> 30 nodes.

# Whether to use a learning rate scheduler and warm-up.
USE_SCHEDULER: False
# Whether to compute mean average precision
COMPUTE_MEAN_AVERAGE_PRECISION: False

# ------------------------------------- DATASET -------------------------------------
# Dataset names.
DATASET: "VOCDataset" # States what main dataset we used to train. Used when saving model checkpoints.
TRAIN_DIR_NAME: "VOC_2012_dataset/train" # Name of the directory that contains the train set.
VALIDATION_DIR_NAME: "VOC_2012_dataset/val"
TEST_DIR_NAME: "VOC2012_test"
IMAGES_DIR_NAME: "JPEGImages" # Name of the directory that contains the images
ANNOTATIONS_DIR_NAME: "Annotations"

IMAGE_SIZE: 448 # 448x448
# prettier-ignore
CLASS_NAMES: [ "person", "bird", "cat", "cow", "dog", "horse", "sheep", "aeroplane", "bicycle", "boat", "bus", "car", "motorbike", "train", "bottle", "chair", "diningtable", "pottedplant", "sofa", "tvmonitor"]

# The number of samples (images/annotations) to grab from the dataset and create a dataframe.
#   Note: error will occur if BATCH_SIZE > NUM_TRAIN_SAMPLES.
NUM_TRAIN_SAMPLES: 10 # Set to 0 -> to create a dataframe of the entire dataset.
NUM_VAL_SAMPLES: 10 # Same applies to NUM_VAL_SAMPLES as NUM_TRAIN_SAMPLES.

